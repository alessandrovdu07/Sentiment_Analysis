{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4ysJPK_noH7"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/python3.7\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from argparse import ArgumentTypeError, ArgumentParser\n",
        "from nltk import tokenize, sent_tokenize, stem\n",
        "from datetime import timedelta\n",
        "from math import ceil, log\n",
        "import preprocessor as p\n",
        "from uuid import uuid4\n",
        "from sys import stdout\n",
        "from os import path\n",
        "import logging\n",
        "import codecs\n",
        "import time\n",
        "import re\n",
        "\n",
        "\n",
        "def show_progress(iteration, total, estimation, prefix='   ', decimals=1, final=False):\n",
        "    \"\"\" Print iterations progress:\n",
        "\n",
        "    :param iteration:\n",
        "    :param total:\n",
        "    :param estimation:\n",
        "    :param prefix:\n",
        "    :param decimals:\n",
        "    :param final:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    columns = 32\n",
        "    eta = str(timedelta(seconds=max(0, int(ceil(estimation)))))\n",
        "    bar_length = int(columns)\n",
        "    str_format = \"{0:.\" + str(decimals) + \"f}\"\n",
        "    percents = str_format.format(100 * (iteration / float(total)))\n",
        "    filled_length = int(round(bar_length * iteration / float(total)))\n",
        "    bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)\n",
        "    stdout.write('\\r%s %s%s |%s| %s' % (prefix, percents, '%', bar, eta))\n",
        "\n",
        "    if final:\n",
        "        stdout.write('\\n')\n",
        "\n",
        "    stdout.flush()\n",
        "\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\" Format a value in seconds to \"day, HH:mm:ss:\n",
        "\n",
        "    :param seconds:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return str(timedelta(seconds=max(0, int(ceil(seconds)))))\n",
        "\n",
        "\n",
        "def str_to_bool(v):\n",
        "    \"\"\" Convert a string value to boolean:\n",
        "\n",
        "    :param v:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise ArgumentTypeError(\"invalid boolean value: %s\" % str(v))\n",
        "\n",
        "\n",
        "def natural(v):\n",
        "    \"\"\" Verify if a value correspond to a natural number (it's an integer and bigger than 0):\n",
        "\n",
        "    :param v:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    try:\n",
        "        v = int(v)\n",
        "\n",
        "        if v > 0:\n",
        "            return v\n",
        "        else:\n",
        "            raise ArgumentTypeError(\"invalid natural number value: '%s'\" % str(v))\n",
        "    except ValueError:\n",
        "        raise ArgumentTypeError(\"invalid natural number value: '%s'\" % str(v))\n",
        "\n",
        "\n",
        "def is_commom_word(word):\n",
        "    \"\"\" Verify if a string correspond to a common word (just digits, letters, hyphens and underlines):\n",
        "\n",
        "    :param word:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if len(word) <= 3 or not any(l.isalpha() for l in word):\n",
        "        return False\n",
        "\n",
        "    return all(l.isalpha() or bool(re.search(\"[A-Za-z0-9-_\\']+\", l)) for l in word)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Defining script arguments:\n",
        "    parser = ArgumentParser(description=\"BoW-Based Text Representation Generator\")\n",
        "    parser._action_groups.pop()\n",
        "    required = parser.add_argument_group('required arguments')\n",
        "    optional = parser.add_argument_group('optional arguments')\n",
        "    optional.add_argument(\"--log\", metavar='BOOL', type=str_to_bool, action=\"store\", dest=\"log\", nargs=\"?\", const=True,\n",
        "                          default=False, required=False, help='display log during the process: y, [N]')\n",
        "    optional.add_argument(\"--tokenize\", metavar='BOOL', type=str_to_bool, action=\"store\", dest=\"tokenize\", nargs=\"?\",\n",
        "                          const=True, default=False, required=False,\n",
        "                          help='specify if texts need to be tokenized: y, [N]')\n",
        "    optional.add_argument(\"--ignore_case\", metavar='BOOL', type=str_to_bool, action=\"store\", dest=\"ignore_case\",\n",
        "                          nargs=\"?\", const=True, default=True, required=False, help='ignore case: [Y], n')\n",
        "    optional.add_argument(\"--stemm\", metavar='BOOL', type=str_to_bool, action=\"store\", dest=\"stemm\", nargs=\"?\",\n",
        "                          const=True, default=False, required=False, help='enable stemming (case insensitive): y, [N]')\n",
        "    optional.add_argument(\"--validate_words\", metavar='BOOL', type=str_to_bool, action=\"store\", dest=\"validate_words\",\n",
        "                          nargs=\"?\", const=True, default=True, required=False,\n",
        "                          help='validate vocabulary ([A-Za-z0-9-_\\']+): [Y], n')\n",
        "    optional.add_argument(\"--stoplist\", metavar='FILE_PATH', type=str, action=\"store\", dest=\"stoplist\", default=None,\n",
        "                          required=False, nargs=\"?\", const=True, help='stoplist file')\n",
        "    optional.add_argument(\"--doc_freq\", metavar='INT', type=natural, action=\"store\", dest=\"doc_freq\", default=2,\n",
        "                          nargs=\"?\", const=True, required=False, help='min. frequency of documents (>= 1): [2]')\n",
        "    optional.add_argument(\"--metric\", metavar='STR', type=str, action=\"store\", dest=\"metric\", default=\"TF-IDF\",\n",
        "                          nargs=\"?\", const=True, required=False, help='term relevance metric: tf, idf, [TF-IDF]')\n",
        "    optional.add_argument(\"--print_features\", metavar='BOOL', type=str_to_bool, action=\"store\", dest=\"print_features\",\n",
        "                          nargs=\"?\", const=True, default=True, required=False,\n",
        "                          help='print features on file header: [Y], n')\n",
        "    required.add_argument(\"--language\", metavar='STR', type=str, action=\"store\", dest=\"language\", nargs=\"?\", const=True,\n",
        "                          required=True, help='dataset language: EN, ES, FR, DE, IT, PT')\n",
        "    required.add_argument(\"--input\", \"-i\", metavar='FILE_PATH', type=str, action=\"store\", dest=\"input\", required=True,\n",
        "                          nargs=\"?\", const=True, help='dataset input file')\n",
        "    required.add_argument(\"--output\", \"-o\", metavar='FILE_PATH', type=str, action=\"store\", dest=\"output\", required=True,\n",
        "                          nargs=\"?\", const=True, help='text representation output file')\n",
        "    args = parser.parse_args()  # Verifying arguments.\n",
        "\n",
        "    # Setup logging:\n",
        "    if args.log:\n",
        "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "    if args.language == \"ES\":  # Spanish.\n",
        "        nltk_language = \"spanish\"\n",
        "        stemmer = stem.snowball.SpanishStemmer()\n",
        "    elif args.language == \"FR\":  # French.\n",
        "        nltk_language = \"french\"\n",
        "        stemmer = stem.snowball.FrenchStemmer()\n",
        "    elif args.language == \"DE\":  # Deutsch.\n",
        "        nltk_language = \"german\"\n",
        "        stemmer = stem.snowball.GermanStemmer()\n",
        "    elif args.language == \"IT\":  # Italian.\n",
        "        nltk_language = \"italian\"\n",
        "        stemmer = stem.snowball.ItalianStemmer()\n",
        "    elif args.language == \"PT\":  # Portuguese.\n",
        "        nltk_language = \"portuguese\"\n",
        "        stemmer = stem.snowball.PortugueseStemmer()\n",
        "    else:  # English.\n",
        "        args.language = \"EN\"\n",
        "        nltk_language = \"english\"\n",
        "        stemmer = stem.snowball.EnglishStemmer()\n",
        "\n",
        "    p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.NUMBER)\n",
        "    total_start = time.time()\n",
        "\n",
        "    ################################################################################\n",
        "    ### INPUT                                                                    ###\n",
        "    ################################################################################\n",
        "\n",
        "    log_file = codecs.open(\"logs/BoW-log_\" + time.strftime(\"%Y-%m-%d\") + \"_\" + time.strftime(\"%H-%M-%S\") + \"_\" +\n",
        "                           str(uuid4().hex) + \".txt\", \"w\", \"utf-8\")\n",
        "    print(\"\\nBoW-Based Text Representation Generator\\n=======================================\\n\")\n",
        "    log_file.write(\"BoW-Based Text Representation Generator\\n=======================================\\n\\n\")\n",
        "    log_file.write(\"> Parameters:\\n\")\n",
        "\n",
        "    if args.tokenize:\n",
        "        log_file.write(\"\\t- Tokenize:\\t\\t\\tyes\\n\")\n",
        "    else:\n",
        "        log_file.write(\"\\t- Tokenize:\\t\\t\\tno\\n\")\n",
        "\n",
        "    if args.ignore_case:\n",
        "        log_file.write(\"\\t- Ignore case:\\t\\tyes\\n\")\n",
        "    else:\n",
        "        log_file.write(\"\\t- Ignore case:\\t\\tno\\n\")\n",
        "\n",
        "    if args.stemm:\n",
        "        log_file.write(\"\\t- Stemming:\\t\\t\\tyes\\n\")\n",
        "    else:\n",
        "        log_file.write(\"\\t- Stemming:\\t\\t\\tno\\n\")\n",
        "\n",
        "    if args.validate_words:\n",
        "        log_file.write(\"\\t- Validate words:\\tyes\\n\")\n",
        "    else:\n",
        "        log_file.write(\"\\t- Validate words:\\tno\\n\")\n",
        "\n",
        "    if args.stoplist is not None:\n",
        "        log_file.write(\"\\t- Stoplist:\\t\\t\" + args.stoplist + \"\\n\")\n",
        "\n",
        "    log_file.write(\"\\t- Doc. frequency:\\t>= \" + str(args.doc_freq) + \"\\n\")\n",
        "    args.metric = args.metric.lower()\n",
        "\n",
        "    if args.metric == \"tf\":\n",
        "        log_file.write(\"\\t- Metric:\\t\\t\\tTF\\n\")\n",
        "    elif args.metric == \"idf\":\n",
        "        log_file.write(\"\\t- Metric:\\t\\t\\tIDF\\n\")\n",
        "    else:\n",
        "        args.metric = \"tf-idf\"\n",
        "        log_file.write(\"\\t- Metric:\\t\\t\\tTF-IDF\\n\")\n",
        "\n",
        "    if args.print_features:\n",
        "        log_file.write(\"\\t- Print features:\\tyes\\n\")\n",
        "    else:\n",
        "        log_file.write(\"\\t- Print features:\\tno\\n\")\n",
        "\n",
        "    log_file.write(\"\\t- Language:\\t\\t\\t\" + args.language + \"\\n\")\n",
        "    log_file.write(\"\\t- Input:\\t\\t\\t\" + args.input + \"\\n\")\n",
        "    log_file.write(\"\\t- Output:\\t\\t\\t\" + args.output + \"\\n\\n\\n\")\n",
        "\n",
        "    if not path.exists(args.input):\n",
        "        print(\"ERROR: input file does not exists!\\n\\t!Filepath: \" + args.input)\n",
        "        log_file.write(\"ERROR: input file does not exists!\\n\\t!Filepath: \" + args.input)\n",
        "        log_file.close()\n",
        "        return\n",
        "\n",
        "    print(\"> Loading dataset...\\n\\n\")\n",
        "    num_samples = sum(1 for line in open(args.input, encoding=\"utf8\")) - 1  # Ignoring header line.\n",
        "    stoplist = []\n",
        "    log_file.write(\"> Dataset filepath:\\t\\t\" + args.input + \"\\n\")\n",
        "\n",
        "    if args.stoplist is not None:\n",
        "        print(\"> Loading stoplist...\\n\\n\\n\")\n",
        "        stoplist_file = codecs.open(args.stoplist, \"r\", encoding='utf-8')\n",
        "\n",
        "        for line in stoplist_file.readlines():\n",
        "            stoplist.append(line.strip())\n",
        "\n",
        "        if args.ignore_case:\n",
        "            stoplist = [w.lower() for w in stoplist]\n",
        "\n",
        "        stoplist.sort()\n",
        "        stoplist_file.close()\n",
        "\n",
        "    ################################################################################\n",
        "    ### PRE-PROCESSING                                                           ###\n",
        "    ################################################################################\n",
        "\n",
        "    print(\"> Building text representation:\")\n",
        "    total_operations = num_samples\n",
        "    num_paragraphs = 0\n",
        "    num_sentences = 0\n",
        "    filepath_i = 0\n",
        "    eta = 0\n",
        "    show_progress(filepath_i, total_operations, eta)\n",
        "    operation_start = time.time()\n",
        "    document_words = []\n",
        "    dataset = codecs.open(args.input, \"r\", \"UTF-8\")\n",
        "    classes_names = [f.strip() for f in dataset.readline().split('\\t')[2:]]\n",
        "\n",
        "    for sample in dataset:\n",
        "        data_columns = sample.replace(\"'\", \"\").split('\\t')\n",
        "        sample_id = data_columns[0].strip()\n",
        "        sample_text = data_columns[1].strip()\n",
        "        sample_classes = [c.strip() for c in data_columns[2:]]\n",
        "        paragraphs = [p.strip() for p in p.clean(sample_text).replace(\"#\", \"\").split()]  # Removing URLs and emojis.\n",
        "        words = []\n",
        "        num_paragraphs += len(paragraphs)\n",
        "\n",
        "        for paragraph in paragraphs:\n",
        "            sentences = sent_tokenize(paragraph, nltk_language)  # Identifying sentences.\n",
        "            num_sentences += len(sentences)\n",
        "\n",
        "            for sentence in sentences:\n",
        "                if args.tokenize:\n",
        "                    tokens = tokenize.word_tokenize(sentence)  # Works well for many European languages.\n",
        "                else:\n",
        "                    tokens = sentence.split()\n",
        "\n",
        "                if args.ignore_case:\n",
        "                    tokens = [t.lower() for t in tokens]\n",
        "\n",
        "                if args.validate_words:\n",
        "                    allowed_tokens = [t for t in tokens if\n",
        "                                      is_commom_word(t) and t not in stoplist]  # Filter allowed tokens.\n",
        "                else:\n",
        "                    allowed_tokens = [t for t in tokens if t not in stoplist]  # Filter allowed tokens.\n",
        "\n",
        "                for token in allowed_tokens:\n",
        "                    if args.stemm:\n",
        "                        new_word = stemmer.stem(token)\n",
        "                    else:\n",
        "                        new_word = token\n",
        "\n",
        "                    if new_word not in stoplist:\n",
        "                        words.append(new_word)\n",
        "\n",
        "        document_words.append({\"id\": sample_id, \"length\": len(words), \"words\": sorted(words),\n",
        "                               \"classes\": sample_classes})  # Sorted words.\n",
        "\n",
        "    dataset.close()\n",
        "\n",
        "    ############################################################################\n",
        "    ### TEXT REPRESENTATION                                                  ###\n",
        "    ############################################################################\n",
        "\n",
        "    bag = []\n",
        "    features = []\n",
        "\n",
        "    for words in document_words:\n",
        "        start = time.time()\n",
        "        bag.append([])  # New frequencies document line.\n",
        "\n",
        "        # Reading all words:\n",
        "        for word in words[\"words\"]:\n",
        "            new_word = True\n",
        "\n",
        "            # Checking if the word has already been inserted in features list (BoW column):\n",
        "            for feature_i, feature in enumerate(features):\n",
        "                if word == feature:\n",
        "                    new_word = False\n",
        "                    new_cell = True\n",
        "\n",
        "                    # Searching where is it the correspondent column of current 'feature' of this line (bag[-1]):\n",
        "                    for item_i, item in enumerate(bag[-1]):\n",
        "                        if item['index'] == feature_i:\n",
        "                            new_cell = False\n",
        "                            bag[-1][item_i]['freq'] += 1\n",
        "                            break\n",
        "\n",
        "                    # Adding new correspondent item in this line:\n",
        "                    if new_cell:\n",
        "                        bag[-1].append({'index': feature_i, 'freq': 1})\n",
        "\n",
        "                    break\n",
        "\n",
        "            # Adding new word in features list (BoW column):\n",
        "            if new_word:\n",
        "                features.append(word)\n",
        "                bag[-1].append({'index': len(features) - 1, 'freq': 1})\n",
        "\n",
        "        filepath_i += 1\n",
        "        end = time.time()\n",
        "        eta = (total_operations - filepath_i) * (end - start)\n",
        "        show_progress(filepath_i, total_operations, eta)\n",
        "\n",
        "    ############################################################################\n",
        "    ### FEATURES RELEVANCE                                                   ###\n",
        "    ############################################################################\n",
        "\n",
        "    bow = []\n",
        "    num_features = len(features)\n",
        "    output_file = codecs.open(args.output, \"w\", encoding='utf-8')\n",
        "    doc_occurences = [0] * num_features\n",
        "\n",
        "    for doc in bag:  # Literal frequency.\n",
        "        bow.append([0.0] * num_features)\n",
        "\n",
        "        for cell in doc:\n",
        "            bow[-1][cell['index']] = cell['freq']\n",
        "            doc_occurences[cell['index']] += 1.0\n",
        "\n",
        "    if args.metric == \"tf\":  # Term Frequency.\n",
        "        for doc_i, doc in enumerate(bow):\n",
        "            for freq_j, freq in enumerate(doc):\n",
        "                if document_words[doc_i][\"length\"] > 0:\n",
        "                    bow[doc_i][freq_j] /= document_words[doc_i][\"length\"]\n",
        "                else:\n",
        "                    bow[doc_i][freq_j] = 0.0\n",
        "    elif args.metric == \"idf\":  # Inverse Document Frequency.\n",
        "        for doc_i, doc in enumerate(bow):\n",
        "            for freq_j, freq in enumerate(doc):\n",
        "                if doc_occurences[freq_j] > 0:\n",
        "                    bow[doc_i][freq_j] = log(num_samples / doc_occurences[freq_j])\n",
        "                else:\n",
        "                    bow[doc_i][freq_j] = 0.0\n",
        "    else:  # Term Frequency - Inverse Document Frequency:\n",
        "        for doc_i, doc in enumerate(bow):\n",
        "            for freq_j, freq in enumerate(doc):\n",
        "                if document_words[doc_i][\"length\"] > 0:\n",
        "                    bow[doc_i][freq_j] = (bow[doc_i][freq_j] / document_words[doc_i][\"length\"]) * \\\n",
        "                                         log(num_samples / doc_occurences[freq_j])\n",
        "                else:\n",
        "                    bow[doc_i][freq_j] = 0.0\n",
        "\n",
        "    # Removing features with frequencies less than specified document frequency:\n",
        "    del_indexes = []\n",
        "\n",
        "    for feature_i, doc_occurence in enumerate(doc_occurences):\n",
        "        if doc_occurence < args.doc_freq:\n",
        "            del_indexes.append(feature_i)\n",
        "\n",
        "    for index in sorted(del_indexes, reverse=True):\n",
        "        del features[index]\n",
        "\n",
        "        for doc in bow:\n",
        "            del doc[index]\n",
        "\n",
        "    num_features = len(features)\n",
        "\n",
        "    ############################################################################\n",
        "    ### OUTPUT                                                               ###\n",
        "    ############################################################################\n",
        "\n",
        "    output_file.write(str(num_samples) + \" \" + str(num_features) + \"\\n\")\n",
        "\n",
        "    if args.print_features:\n",
        "        output_file.write(str(\"\\t\".join(features)) + \"\\t\")\n",
        "    else:\n",
        "        output_file.write(str(\"\\t\".join([\"f\" + str(i) for i in range(1, num_features + 1)])) + \"\\t\")\n",
        "\n",
        "    output_file.write(str(\"\\t\".join(classes_names)) + \"\\n\")\n",
        "\n",
        "    for doc_i, doc in enumerate(bow):\n",
        "        for freq in doc:\n",
        "            output_file.write(str(freq) + \"\\t\")\n",
        "\n",
        "        output_file.write(str(\"\\t\".join(document_words[doc_i][\"classes\"])) + \"\\n\")\n",
        "\n",
        "    output_file.close()\n",
        "    operation_end = time.time()\n",
        "    eta = operation_end - operation_start\n",
        "    show_progress(total_operations, total_operations, eta, final=True)\n",
        "\n",
        "    total_end = time.time()\n",
        "    run_time = format_time(total_end - total_start)\n",
        "\n",
        "    print(\"\\n\\n> Log:\")\n",
        "    print(\"    - Run time:\\t\\t%s\" % run_time)\n",
        "    print(\"    - # samples:\\t%i\" % num_samples)\n",
        "    print(\"    - # features:\\t%i\" % num_features)\n",
        "    print(\"    - # paragraphs:\\t%i\" % num_paragraphs)\n",
        "    print(\"    - # sentences:\\t%i\\n\" % num_sentences)\n",
        "\n",
        "    log_file.write(\"\\n\\n> Log:\\n\")\n",
        "    log_file.write(\"\\t- Run time:\\t\\t\\t%s\\n\" % run_time)\n",
        "    log_file.write(\"\\t- # samples:\\t\\t%i\\n\" % num_samples)\n",
        "    log_file.write(\"\\t- # features:\\t\\t%i\\n\" % num_features)\n",
        "    log_file.write(\"\\t- # paragraphs:\\t\\t%i\\n\" % num_paragraphs)\n",
        "    log_file.write(\"\\t- # sentences:\\t\\t%i\" % num_sentences)\n",
        "    log_file.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}